{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline No-Show Optimizer - Model Development\n",
    "\n",
    "This notebook develops and evaluates machine learning models to predict passenger no-show probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from data_preprocessing import DataPreprocessor\n",
    "from feature_engineering import FeatureEngineer\n",
    "from models import ModelTrainer\n",
    "from optimization import RevenueOptimizer\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from EDA\n",
    "# data_path = '../data/processed/airline_data_processed.csv'\n",
    "# df = pd.read_csv(data_path)\n",
    "\n",
    "# For demonstration, show expected data structure\n",
    "print(\"Please ensure your processed data is available at '../data/processed/airline_data_processed.csv'\")\n",
    "print(\"Or run the exploratory analysis notebook first to generate the processed data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and feature engineer\n",
    "preprocessor = DataPreprocessor()\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Clean and prepare data\n",
    "# df_clean = preprocessor.clean_data(df)\n",
    "# print(f\"Data shape after cleaning: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive feature set\n",
    "# df_features = feature_engineer.create_all_features(df_clean)\n",
    "# print(f\"Data shape after feature engineering: {df_features.shape}\")\n",
    "\n",
    "# Display new feature columns\n",
    "# new_features = set(df_features.columns) - set(df_clean.columns)\n",
    "# print(f\"\\nNew features created: {len(new_features)}\")\n",
    "# print(list(new_features)[:20])  # Show first 20 new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select most important features\n",
    "# target_column = 'no_show'\n",
    "# important_features = feature_engineer.select_features(df_features, target_column)\n",
    "# print(f\"Selected {len(important_features)} important features\")\n",
    "# print(important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# X = df_features[important_features]\n",
    "# y = df_features[target_column]\n",
    "\n",
    "# Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# print(f\"Training set size: {X_train.shape}\")\n",
    "# print(f\"Test set size: {X_test.shape}\")\n",
    "# print(f\"No-show rate in training: {y_train.mean():.3f}\")\n",
    "# print(f\"No-show rate in test: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer()\n",
    "\n",
    "# Train all models\n",
    "# models = model_trainer.train_all_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Display model performance\n",
    "# performance_summary = pd.DataFrame({\n",
    "#     'Model': list(models.keys()),\n",
    "#     'Train_Score': [models[m]['train_score'] for m in models.keys()],\n",
    "#     'Test_Score': [models[m]['test_score'] for m in models.keys()]\n",
    "# })\n",
    "# \n",
    "# print(\"Model Performance Summary:\")\n",
    "# print(performance_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model\n",
    "# best_model_name = performance_summary.loc[performance_summary['Test_Score'].idxmax(), 'Model']\n",
    "# best_model = models[best_model_name]\n",
    "\n",
    "# print(f\"Best model: {best_model_name}\")\n",
    "# print(f\"Test accuracy: {best_model['test_score']:.4f}\")\n",
    "\n",
    "# # Detailed metrics\n",
    "# y_pred = best_model['predictions']\n",
    "# y_pred_proba = best_model['probabilities']\n",
    "# \n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# \n",
    "# print(f\"\\nAUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# # ROC Curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "# auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "# \n",
    "# axes[0,0].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "# axes[0,0].plot([0, 1], [0, 1], 'k--')\n",
    "# axes[0,0].set_xlabel('False Positive Rate')\n",
    "# axes[0,0].set_ylabel('True Positive Rate')\n",
    "# axes[0,0].set_title('ROC Curve')\n",
    "# axes[0,0].legend()\n",
    "\n",
    "# # Confusion Matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# sns.heatmap(cm, annot=True, fmt='d', ax=axes[0,1], cmap='Blues')\n",
    "# axes[0,1].set_xlabel('Predicted')\n",
    "# axes[0,1].set_ylabel('Actual')\n",
    "# axes[0,1].set_title('Confusion Matrix')\n",
    "\n",
    "# # Prediction Distribution\n",
    "# axes[1,0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='No Show = 0', density=True)\n",
    "# axes[1,0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='No Show = 1', density=True)\n",
    "# axes[1,0].set_xlabel('Predicted Probability')\n",
    "# axes[1,0].set_ylabel('Density')\n",
    "# axes[1,0].set_title('Prediction Distribution')\n",
    "# axes[1,0].legend()\n",
    "\n",
    "# # Feature Importance (if available)\n",
    "# if 'feature_importance' in best_model:\n",
    "#     importance = best_model['feature_importance']\n",
    "#     top_features = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15])\n",
    "#     \n",
    "#     feature_names = list(top_features.keys())\n",
    "#     feature_values = list(top_features.values())\n",
    "#     \n",
    "#     axes[1,1].barh(range(len(feature_names)), feature_values)\n",
    "#     axes[1,1].set_yticks(range(len(feature_names)))\n",
    "#     axes[1,1].set_yticklabels(feature_names)\n",
    "#     axes[1,1].set_xlabel('Importance')\n",
    "#     axes[1,1].set_title('Top 15 Feature Importances')\n",
    "# else:\n",
    "#     axes[1,1].text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "#                    ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "#     axes[1,1].set_title('Feature Importance')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for best model\n",
    "# print(\"Performing hyperparameter tuning...\")\n",
    "# tuning_results = model_trainer.hyperparameter_tuning(X_train, y_train, best_model_name)\n",
    "\n",
    "# print(f\"\\nBest parameters: {tuning_results['best_params']}\")\n",
    "# print(f\"Best cross-validation score: {tuning_results['best_score']:.4f}\")\n",
    "\n",
    "# # Evaluate tuned model\n",
    "# tuned_model = tuning_results['best_model']\n",
    "# tuned_predictions = tuned_model.predict(X_test)\n",
    "# tuned_probabilities = tuned_model.predict_proba(X_test)[:, 1]\n",
    "# tuned_accuracy = tuned_model.score(X_test, y_test)\n",
    "# tuned_auc = roc_auc_score(y_test, tuned_probabilities)\n",
    "\n",
    "# print(f\"\\nTuned model performance:\")\n",
    "# print(f\"Accuracy: {tuned_accuracy:.4f}\")\n",
    "# print(f\"AUC: {tuned_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "# if 'feature_importance' in best_model:\n",
    "#     feature_importance = model_trainer.get_feature_importance(best_model_name, top_n=20)\n",
    "#     \n",
    "#     importance_df = pd.DataFrame({\n",
    "#         'Feature': list(feature_importance.keys()),\n",
    "#         'Importance': list(feature_importance.values())\n",
    "#     })\n",
    "#     \n",
    "#     print(\"Top 20 Most Important Features:\")\n",
    "#     print(importance_df.round(4))\n",
    "#     \n",
    "#     # Visualize feature importance\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.barh(range(len(importance_df)), importance_df['Importance'])\n",
    "#     plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "#     plt.xlabel('Feature Importance')\n",
    "#     plt.title('Top 20 Feature Importances')\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance across different passenger segments\n",
    "# test_data = X_test.copy()\n",
    "# test_data['actual_no_show'] = y_test\n",
    "# test_data['predicted_no_show'] = y_pred\n",
    "# test_data['predicted_probability'] = y_pred_proba\n",
    "\n",
    "# # Performance by probability ranges\n",
    "# test_data['risk_category'] = pd.cut(test_data['predicted_probability'], \n",
    "#                                     bins=[0, 0.1, 0.3, 0.7, 1.0],\n",
    "#                                     labels=['Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk'])\n",
    "\n",
    "# risk_analysis = test_data.groupby('risk_category').agg({\n",
    "#     'actual_no_show': ['count', 'mean'],\n",
    "#     'predicted_probability': 'mean'\n",
    "# }).round(3)\n",
    "\n",
    "# risk_analysis.columns = ['count', 'actual_no_show_rate', 'predicted_probability']\n",
    "# print(\"Model Performance by Risk Category:\")\n",
    "# print(risk_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Revenue Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize revenue optimizer\n",
    "revenue_optimizer = RevenueOptimizer()\n",
    "\n",
    "# Example optimization for a specific flight\n",
    "# flight_capacity = 180\n",
    "# sample_flight_data = test_data.sample(150).copy()  # Simulate a flight with 150 bookings\n",
    "# sample_flight_data['no_show_probability'] = sample_flight_data['predicted_probability']\n",
    "# sample_flight_data['ticket_price'] = np.random.normal(400, 150, len(sample_flight_data))  # Sample prices\n",
    "\n",
    "# # Optimize overbooking\n",
    "# optimization_results = revenue_optimizer.optimize_overbooking_rate(\n",
    "#     sample_flight_data, flight_capacity\n",
    "# )\n",
    "\n",
    "# print(\"Optimization Results:\")\n",
    "# for key, value in optimization_results.items():\n",
    "#     if isinstance(value, float):\n",
    "#         print(f\"{key}: {value:.4f}\")\n",
    "#     else:\n",
    "#         print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "# model_save_path = '../results/best_model.pkl'\n",
    "# model_trainer.save_model(best_model_name, model_save_path)\n",
    "# print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save feature list for deployment\n",
    "# feature_list_path = '../results/feature_list.txt'\n",
    "# with open(feature_list_path, 'w') as f:\n",
    "#     for feature in important_features:\n",
    "#         f.write(f\"{feature}\\n\")\n",
    "# print(f\"Feature list saved to {feature_list_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL PERFORMANCE SUMMARY ===\")\n",
    "print(\"Best model: [TO BE DETERMINED]\")\n",
    "print(\"Test accuracy: [TO BE CALCULATED]\")\n",
    "print(\"AUC score: [TO BE CALCULATED]\")\n",
    "print(\"\")\n",
    "print(\"=== KEY FEATURES ===\")\n",
    "print(\"Most important predictors: [TO BE IDENTIFIED]\")\n",
    "print(\"\")\n",
    "print(\"=== BUSINESS IMPACT ===\")\n",
    "print(\"Expected revenue improvement: [TO BE CALCULATED]\")\n",
    "print(\"Optimal overbooking rate: [TO BE OPTIMIZED]\")\n",
    "print(\"\")\n",
    "print(\"=== NEXT STEPS ===\")\n",
    "print(\"1. Deploy model to production environment\")\n",
    "print(\"2. Implement real-time prediction pipeline\")\n",
    "print(\"3. Monitor model performance and retrain as needed\")\n",
    "print(\"4. A/B test optimization strategies\")\n",
    "print(\"5. Expand to additional routes and airlines\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}